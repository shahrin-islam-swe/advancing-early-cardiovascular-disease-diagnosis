{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea3pUmONxNFC"
      },
      "outputs": [],
      "source": [
        "# === SECTION 1: 2 BEST MODELS ENSEMBLE EVALUATION  ===\n",
        "\n",
        "# Weighted\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from imblearn.combine import SMOTETomek\n",
        "from imblearn.over_sampling import ADASYN\n",
        "\n",
        "# --- SETUP (Same as before) ---\n",
        "df = pd.read_csv('heart.csv')\n",
        "le = LabelEncoder()\n",
        "for col in ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']:\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "X = df.drop('HeartDisease', axis=1)\n",
        "y = df['HeartDisease']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=369)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --- TRAIN MODELS ON THEIR SPECIFIC DATA ---\n",
        "# 1. LightGBM (SMOTETomek)\n",
        "smt = SMOTETomek(random_state=369)\n",
        "X_train_lgbm, y_train_lgbm = smt.fit_resample(X_train_scaled, y_train)\n",
        "model_lgbm = LGBMClassifier(random_state=369, verbose=-1).fit(X_train_lgbm, y_train_lgbm)\n",
        "\n",
        "# 2. XGBoost (ADASYN)\n",
        "ada = ADASYN(random_state=369)\n",
        "X_train_xgb, y_train_xgb = ada.fit_resample(X_train_scaled, y_train)\n",
        "model_xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=369).fit(X_train_xgb, y_train_xgb)\n",
        "\n",
        "# --- WEIGHTED VOTING LOOP ---\n",
        "print(f\"{'LGBM Weight':<12} | {'XGB Weight':<12} | {'Accuracy':<10}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "probs_lgbm = model_lgbm.predict_proba(X_test_scaled)\n",
        "probs_xgb = model_xgb.predict_proba(X_test_scaled)\n",
        "\n",
        "best_acc = 0\n",
        "best_w = (0, 0)\n",
        "\n",
        "# Try weights from 0.0 to 1.0\n",
        "for w_lgbm in np.arange(0.1, 1.0, 0.1):\n",
        "    w_xgb = 1.0 - w_lgbm\n",
        "\n",
        "    # Weighted Average\n",
        "    avg_probs = (probs_lgbm * w_lgbm) + (probs_xgb * w_xgb)\n",
        "    y_pred = np.argmax(avg_probs, axis=1)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"{w_lgbm:.1f}          | {w_xgb:.1f}          | {acc:.4%}\")\n",
        "\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        best_w = (w_lgbm, w_xgb)\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(f\"üèÜ BEST WEIGHTS: {best_w[0]:.1f} LGBM / {best_w[1]:.1f} XGB -> {best_acc:.4%}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === SECTION 2: PROPOSED METHOD (LIGHTGBM 0.8 + XGBOOST 0.2) ===\n",
        "\n",
        "# Weighted (lgbm 0.6,xgboost 0.4)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from imblearn.combine import SMOTETomek\n",
        "from imblearn.over_sampling import ADASYN\n",
        "\n",
        "# 1. Load & Preprocess Data\n",
        "df = pd.read_csv('heart.csv')\n",
        "\n",
        "le = LabelEncoder()\n",
        "categorical_cols = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
        "for col in categorical_cols:\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "X = df.drop('HeartDisease', axis=1)\n",
        "y = df['HeartDisease']\n",
        "\n",
        "# 2. Split Data (Seed 369)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=369)\n",
        "\n",
        "# 3. Scale Data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 4: TRAIN MODELS ON THEIR SPECIALIZED DATA\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Model A: LightGBM (Using SMOTETomek)\n",
        "print(\"Training LightGBM (w/ SMOTETomek)...\")\n",
        "smt = SMOTETomek(random_state=369)\n",
        "X_train_lgbm, y_train_lgbm = smt.fit_resample(X_train_scaled, y_train)\n",
        "model_lgbm = LGBMClassifier(random_state=369, verbose=-1)\n",
        "model_lgbm.fit(X_train_lgbm, y_train_lgbm)\n",
        "\n",
        "# Model B: XGBoost (Using ADASYN)\n",
        "print(\"Training XGBoost (w/ ADASYN)...\")\n",
        "ada = ADASYN(random_state=369)\n",
        "X_train_xgb, y_train_xgb = ada.fit_resample(X_train_scaled, y_train)\n",
        "model_xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=369)\n",
        "model_xgb.fit(X_train_xgb, y_train_xgb)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 5: APPLY WEIGHTED VOTING (0.8 / 0.2)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Get probabilities for Class 1 (Heart Disease)\n",
        "probs_lgbm = model_lgbm.predict_proba(X_test_scaled)\n",
        "probs_xgb = model_xgb.predict_proba(X_test_scaled)\n",
        "\n",
        "# Apply Weights\n",
        "# Formula: (Prob_LGBM * 0.8) + (Prob_XGB * 0.2)\n",
        "weighted_probs = (probs_lgbm * 0.8) + (probs_xgb * 0.2)\n",
        "\n",
        "# Convert to final prediction (Class with highest score)\n",
        "y_pred_ensemble = np.argmax(weighted_probs, axis=1)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 6: EVALUATE\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "acc_lgbm = accuracy_score(y_test, model_lgbm.predict(X_test_scaled))\n",
        "acc_xgb = accuracy_score(y_test, model_xgb.predict(X_test_scaled))\n",
        "acc_ensemble = accuracy_score(y_test, y_pred_ensemble)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"RESULTS (Seed 369 | Weights: LGBM=0.8, XGB=0.2)\")\n",
        "print(\"=\"*50)\n",
        "print(f\"1. LightGBM (Individual):  {acc_lgbm:.4%}\")\n",
        "print(f\"2. XGBoost  (Individual):  {acc_xgb:.4%}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"üèÜ WEIGHTED ENSEMBLE:       {acc_ensemble:.4%}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if acc_ensemble > max(acc_lgbm, acc_xgb):\n",
        "    print(\"‚úÖ SUCCESS: The 60/40 weighting improved the result!\")\n",
        "elif acc_ensemble == max(acc_lgbm, acc_xgb):\n",
        "    print(\"‚ö†Ô∏è SAME: The result matched the best individual model.\")\n",
        "else:\n",
        "    print(\"‚ùå DECREASE: This weighting combination performed worse.\")"
      ],
      "metadata": {
        "id": "sN3J7YhQxnC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === SECTION 3: CONFUSION MATRIX & ROC Curve (PROPOSED METHOD) ===\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from imblearn.combine import SMOTETomek\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from matplotlib.colors import LinearSegmentedColormap # <--- Added this\n",
        "\n",
        "# 1. Load & Preprocess Data\n",
        "df = pd.read_csv('heart.csv')\n",
        "\n",
        "le = LabelEncoder()\n",
        "categorical_cols = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
        "for col in categorical_cols:\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "X = df.drop('HeartDisease', axis=1)\n",
        "y = df['HeartDisease']\n",
        "\n",
        "# 2. Split Data (Seed 369)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=369)\n",
        "\n",
        "# 3. Scale Data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 4: TRAIN MODELS\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Model A: LightGBM (Using SMOTETomek)\n",
        "smt = SMOTETomek(random_state=369)\n",
        "X_train_lgbm, y_train_lgbm = smt.fit_resample(X_train_scaled, y_train)\n",
        "model_lgbm = LGBMClassifier(random_state=369, verbose=-1)\n",
        "model_lgbm.fit(X_train_lgbm, y_train_lgbm)\n",
        "\n",
        "# Model B: XGBoost (Using ADASYN)\n",
        "ada = ADASYN(random_state=369)\n",
        "X_train_xgb, y_train_xgb = ada.fit_resample(X_train_scaled, y_train)\n",
        "model_xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=369)\n",
        "model_xgb.fit(X_train_xgb, y_train_xgb)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 5: WEIGHTED ENSEMBLE (0.8 / 0.2)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "probs_lgbm = model_lgbm.predict_proba(X_test_scaled)\n",
        "probs_xgb = model_xgb.predict_proba(X_test_scaled)\n",
        "\n",
        "weighted_probs = (probs_lgbm * 0.8) + (probs_xgb * 0.2)\n",
        "y_pred_ensemble = np.argmax(weighted_probs, axis=1)\n",
        "y_prob_positive = weighted_probs[:, 1]\n",
        "\n",
        "acc_ensemble = accuracy_score(y_test, y_pred_ensemble)\n",
        "print(f\"üèÜ WEIGHTED ENSEMBLE ACCURACY: {acc_ensemble:.4%}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 6: PLOTTING (FIXED)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Create a Custom Teal Colormap just for this plot\n",
        "# (White -> Pastel Teal -> Dark Teal)\n",
        "colors_teal = [\"#F2FBF9\", \"#48C9B0\", \"#00796B\"]\n",
        "cmap_custom = LinearSegmentedColormap.from_list(\"CustomTeal\", colors_teal)\n",
        "\n",
        "# Setup Layout\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# --- Plot 1: Confusion Matrix ---\n",
        "cm = confusion_matrix(y_test, y_pred_ensemble)\n",
        "group_names = ['True Negative','False Positive','False Negative','True Positive']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n",
        "group_percentages = [\"{0:.2%}\".format(value) for value in cm.flatten()/np.sum(cm)]\n",
        "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names, group_counts, group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "\n",
        "# Use 'cmap_custom' here instead of 'Teal_r'\n",
        "sns.heatmap(cm, annot=labels, fmt='', cmap=cmap_custom, cbar=False,\n",
        "            ax=axes[0], annot_kws={\"fontsize\":12, \"fontweight\":\"bold\"})\n",
        "axes[0].set_title('Confusion Matrix\\n(Weighted Ensemble)', fontsize=14, fontweight='bold', pad=15)\n",
        "axes[0].set_xlabel('Predicted', fontsize=12)\n",
        "axes[0].set_ylabel('Actual', fontsize=12)\n",
        "axes[0].set_xticklabels(['Normal', 'Heart Disease'])\n",
        "axes[0].set_yticklabels(['Normal', 'Heart Disease'])\n",
        "\n",
        "# --- Plot 2: ROC Curve ---\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob_positive)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "axes[1].plot(fpr, tpr, color='#FF6F61', lw=3, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "axes[1].set_xlim([0.0, 1.0])\n",
        "axes[1].set_ylim([0.0, 1.05])\n",
        "axes[1].set_xlabel('False Positive Rate', fontsize=12)\n",
        "axes[1].set_ylabel('True Positive Rate', fontsize=12)\n",
        "axes[1].set_title('Receiver Operating Characteristic (ROC)', fontsize=14, fontweight='bold', pad=15)\n",
        "axes[1].legend(loc=\"lower right\", fontsize=11)\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3PiRerd4xyrI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}